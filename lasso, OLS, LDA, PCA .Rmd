---
title: "code_elections"
author: ""
date: "3/7/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Installing the packages 


```{r}
library(readxl)
library(MASS)
library(glmnet)
library(tidyverse)
library(broom)
library(glmnet)
```


# LASSO Regression for Feature Selection

# Splitting the data 

In the first step, we split the data into training and test set. 
We use a split of 0.66.

```{r}
# We set a seed for reproducability
set.seed(144)
library(tidyverse)
library(skimr)
library(mlr3)
library(glmnet)
library(plotmo)

# Importing the dataset
#data <- read.csv("~/Elections-Belarus/train_pls1")
data <- read_excel("df_merge3.xlsx", sheet = "Sheet1")

# Remove inf
data = data %>% 
  filter_all(all_vars(!is.infinite(.)))

# Remove columns we dont need 
data = data[-(8:12)]
#data = data[-(21:25)]

colnames(data)[2:14] = c("total","received", "took_part", "earlyvoting","residence","electionday", "vote_dropped_out","against_all","against_onecandiate", "invalid", "commission", "spoiled", " unused")

# Determining number of row and columns
p = ncol(data) -1
n <- nrow(data)

# Split the data
ind_train = sample(x = 1:n, size = ceiling(0.66 * n))
set_train = data[ind_train,]
ind_test = setdiff(x=1:n, ind_train)
set_test = data[ind_test,] 
```

# Brief Summary statistics and exploration

Next we can undertake a brief exploratory data analysis. 


```{r}
# Naive model with all variables 
model_naive = lm(lukashenko_share~., data = set_train)
summary(model_naive)

par(mfrow = c(3, 1))
hist(data$lukashenko_share) 
hist(set_train$lukashenko_share)
hist(set_test$lukashenko_share)

skim(data)
```

Next, we can also look at the correlations between the features. 


```{r}
library(corrplot)
correl = cor(data[-1])
corrplot(correl)
```


```{r}

model_lasso = glmnet(x = as.matrix(set_train[, - (p+1)]), y = set_train$lukashenko_share, alpha =1)

cv<-cv.glmnet(as.matrix(set_train[-(p+1)]), set_train$lukashenko_share, nfolds = 3) 
plot(cv)

lambda_lasso = cv.glmnet(x = as.matrix(set_train[,-(p+1)]),y = set_train$lukashenko_share, alpha =1)$lambda.min

# Plot log lambda
plot_glmnet(x = model_lasso, label = TRUE, xvar = "lambda")
title(main = "LASSO", line = 3)
```

We can first look at how our models perform on our training data.
Here we are comparing a random forest and a lasso model. 

```{r}
y_train = set_train$lukashenko_share
predict_train = matrix(data =0, nrow= nrow(set_train), ncol=2)

predict_train[, 1] = predict(object = model_naive, newdata = set_train[, -(p + 1)])
predict_train[,2] = predict.glmnet(object = model_lasso, 
                                  newx = as.matrix(set_train[,-(p+1)]),
                                  s = lambda_lasso)

colnames(predict_train) =c("Naive Model", "Lasso Model")
```

# Examining MSE for different predictions 

After generating our predictions, we need to calculate our mean squared errors (MSE).

```{r}
MSE_train = rep(x=0, length.out =2) 
for (i in 1:2){
  MSE_train[i] = mean((y_train-predict_train[,i])^2)
}
names(MSE_train) = c("Naive Model", "Lasso Model")
MSE_train
```


Predicting on our training data, we can see that the OLS model without regularization performs better than the Lasso model, although they are somewhat similar. 
Subsequently, we can look at how our model performs when dealing with new data. 
Therefore, we use the test set and again predict our naive and our lasso model.

```{r}
y_test = set_test$lukashenko_share
predict_test = matrix(data =0, nrow= nrow(set_test), ncol=2)

predict_test[, 1] = predict(object = model_naive, newdata = set_test[, -(p + 1)])
predict_test[,2] = predict.glmnet(object = model_lasso,  newx = as.matrix(set_test[,-(p+1)]),
                                  s = lambda_lasso)

colnames(predict_test) =c("Naive Model", "Lasso Model")

MSE_test = rep(x=0, length.out =2) 
for (i in 1:2){
  MSE_test[i] = mean((y_test-predict_test[,i])^2)
}
names(MSE_test) = c("Naive Model", "Lasso Model")
MSE_test
```

# OLS Regression with selected features

Here we can see a very different performance. 
The Lasso model performs substantially better then the OLS model.
Therefore, using this method has led to better predictive performance. 
Lastly, we can also use the lasso specification for a OLS model, trained on test data to ensure that we have no incorrect standard errors.
This is a pivotal concern within in economics, because we need correct standard errors to interpret our coefficients and ensure causality. 

As a first step we can look at the coefficients in the lasso model:

```{r}
coef_lasso <- model_lasso$beta[, which(model_lasso$lambda == lambda_lasso)]
which(coef_lasso!=0)
```

The advantage of the lasso method is that we induce sparsity.
Here we can see that various features turn to 0.
One could also argue that due to correlation between some features, explanatory or variance was ascribed to either the one or the other variable. 
We can see that the variables that were more correlated (see correlation plot above) frequently scored 0 here. 
Now we can specify a model with the relevant coefficients, and run the model on the test data. 

```{r}
model = lm(lukashenko_share ~  earlyvoting  + residence + electday + dropped + against_all + commission + spoiled +  long + army + educ + science + profunion + econ, data = set_test)
summary(model)
```
Here we can see that on the test data some variables still remain insignificant. 
This could be due to the effect that the effect was very weak and barely significant in the other model, too.

```{r}
library(ggfortify)
library(devtools)
install_github("vqv/ggbiplot")
library(ggbiplot)

summary(PCA)
screeplot(PCA, npcs =8, type = "lines")
install.packages("ggfortify")
install.packages("")
autoplot(PCA, colour = "lukashenko_share", loadings =T,loading.label =T,loadings.label.size = 20)
```

# Experimental plots (# TODO: Region variable restructure)

```{r}
PCA = prcomp(data[,19:31], scale. = TRUE,center = TRUE)
summary(PCA)
ggbiplot(PCA,ellipse=TRUE, labels = rownames(data), groups= data$economics)
```

# LDA 



```{r}
library(tidyverse)
library(lda)
library(topicmodels)
library(tidytext)
library(tm)

#data = unite(lits3_gis,col = "text", country:b2c, sep =" ")
#data <- read_csv("~/Downloads/BelarusElections/belarus-qgis.csv")
data <- read_excel("df_merge_n.xlsx", sheet = "Sheet1")
data = unite(data, col = "text", commission_code:economics, sep =" ")

data = data %>%
  select("text")

cleaner <- function(text){
  text <- tolower(text)
  text <- gsub("rt", "", text)
  text <- gsub("@\\w+", "", text)
  text <- gsub("[[:punct:]]", "", text)
  text <- gsub("http\\w+", "", text)
  text <- gsub("amp", " ", text)
  text <- gsub("[ |\t]{2,}", "", text)
  text <- gsub("^ ", "", text)
  text <- gsub(" $", "", text)
  text <- gsub(" +", " ", text)
  text <- gsub("=", " ", text)
  text <- gsub('<.*>', '', enc2native(text))
  text <- unique(text)
  return(text)
}
#####################################################################################

polish <- function(text){
  text <- VCorpus(VectorSource(text))
  text <- tm_map(text, removeWords, stopwords("english"))
  text <- tm_map(text, removeNumbers)
  text <- tm_map(text, stemDocument)
}
text = data$text 
text <- cleaner(text)
corpus <- polish(text)

doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(corpus)))
dtm <- DocumentTermMatrix(corpus[doc.lengths > 0])

#####################################################################################

#LDA <- LDA(dtm, k =3,method = "VEM", cotrol = list(seed=1234))
LDA <- LDA(x=dtm, k=2, method="Gibbs",control=list(alpha=1, delta=0.1, seed=10005))

topics <- tidy(LDA, matrix ="beta")

ap_top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

#####################################################################################

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```
```

