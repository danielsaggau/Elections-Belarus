{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Belarus ML Full Analysis.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjui8RAlcTQhG1VZcDVM1U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/Elections-Belarus/blob/main/Belarus_ML_Full_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUsx07PLrgMa"
      },
      "source": [
        "# Data Cleansing \n",
        "\n",
        "In the first part of this analysis, we need to undertake some pre-processing steps. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOuLBwofEYYc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df_pls=pd.read_excel(\"/df_merge_n.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBryEZuZHKQb"
      },
      "source": [
        "df_pls_pls=df_pls.drop(columns=['Unnamed: 0'])\n",
        "df_pls_pls=df_pls_pls.fillna(0)\n",
        "df_pls_new=df_pls_pls.drop(columns=['commission_code','number_of_cite'])\n",
        "df_pls_new.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnEN3jXtHT5r"
      },
      "source": [
        "df_merge1=df_pls_new.drop(columns=['attachment1','attachment2','attachment3','attachment4','attachment5','comment','id','type','name','location','origin','area'])\n",
        "df_merge1.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAmpDVw6yCTc"
      },
      "source": [
        "# Feature Engineering \n",
        "We can now calculate the share of votes for Lukashenko. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuUEJ6mqHgLG"
      },
      "source": [
        "df_merge1['lukashenko_share']=df_merge1['Lukashenko']/df_merge1['number_of_voters_who_took_part_in_the_voting']\n",
        "df_merge1['lukashenko_share']=df_merge1['lukashenko_share'].replace([np.inf, -np.inf], 0)\n",
        "df_merge1.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4fzMvQbyBeQ"
      },
      "source": [
        "Now, we can drop some of the obsolete columns. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWA9G7OXHnih"
      },
      "source": [
        "df_merge2=df_merge1.drop(columns=['parent_id','description_x','commission_id','work_title','description_y','city_or_district_within_the_region','area_in_the_city','settlement'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkENN5lDHqSw"
      },
      "source": [
        "df_merge2.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp9NQuOxHsiF"
      },
      "source": [
        "df_merge2['region'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZsFOzaYwzZh"
      },
      "source": [
        "Next we need to rename the regions and change no region to 0: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eho6rxJaIw2A"
      },
      "source": [
        "df_merge2['region']=df_merge2['region'].replace('Брестская','brestskaya')\n",
        "df_merge2['region']=df_merge2['region'].replace('Витебская','vitebskaya')\n",
        "df_merge2['region']=df_merge2['region'].replace('Гомельская','gomelskaya')\n",
        "df_merge2['region']=df_merge2['region'].replace('Гродненская','grodnenskaya')\n",
        "df_merge2['region']=df_merge2['region'].replace('Минская','miskaya')\n",
        "df_merge2['region']=df_merge2['region'].replace('Могилевская','mogilevskaya')\n",
        "df_merge2['region']=df_merge2['region'].replace('город Минск','minsk')\n",
        "df_merge2.region=df_merge2.region.replace(0, 'no_region')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfVGGZOHxCL5"
      },
      "source": [
        "Based on this variable, we can craete dummy variables for each region. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HroxKbOfw411"
      },
      "source": [
        "df_merge3=pd.get_dummies(df_merge2, 'region')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqGnIKjYxEhR"
      },
      "source": [
        "We can save this dataset as an intermediate dataset.\n",
        "We also save a version without the region dummies for the regression analysis thereafter, due to the limited number of observations for some regions that could become problematic later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLBsMMaRIzxG"
      },
      "source": [
        "df_merge3.info()\n",
        "df_merge3=df_merge3.fillna(0)\n",
        "df_merge_d=df_merge2.fillna(0)\n",
        "df_merge_d.to_excel(\"df_merge_d.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNz7TbsnxQZY"
      },
      "source": [
        "# Random Forest for Feature Selection \n",
        "\n",
        "In this section we use a random forest for feature selection.\n",
        "This section is structured as follows:\n",
        "\n",
        "1. Specifying our model structure and removing variables that are not needed or variables that should be included for the prediction  \n",
        "2. Splitting the data set into training and test set \n",
        "3. Running the algorithm, using cross validation\n",
        "4. Plot the feature importance per feature "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfwwUmz6Kkz9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold \n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUlMaiaUsWy_"
      },
      "source": [
        "# Splitting the Dataset and specifying our dependent Variable\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RR0M77aQwio"
      },
      "source": [
        "\n",
        "# обязательный код\n",
        "X=df_merge3.drop(columns=['lukashenko_share','Dmitriev','Kanopatskaya','Lukashenko','Tikhanovskaya','Cherechen']).values\n",
        "y=df_merge3.lukashenko_share.values\n",
        "# сплитуйте X и y как хотите\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, \n",
        "                                                    shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2o6YaZnx2CQ"
      },
      "source": [
        "Next we can instantiate our RF regressor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UrHRNrSQ2D3"
      },
      "source": [
        "rf = RandomForestRegressor(random_state = 42)\n",
        "from pprint import pprint\n",
        "# Look at parameters used by our current forest\n",
        "print('Parameters currently in use:\\n')\n",
        "pprint(rf.get_params())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve6f0_mqx9MB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpsSI3nuQ7wa"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1600, num = 8)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 90, num = 9)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "pprint(random_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTQFhSVgQ-uM"
      },
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "rf = RandomForestRegressor()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt-aYEAwqbik"
      },
      "source": [
        "rf_random.best_params_\n",
        "y_pred=rf_random.predict(X_test)\n",
        "print('Правильность на обучающем наборе: {:.5f}'.format(rf_random.score(X_train, y_train)))\n",
        "print('Правильность на тестовом наборе: {:.5f}'.format(rf_random.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7N4IZCSqhdd"
      },
      "source": [
        "importance_values=rf_random.best_estimator_.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9esV4JXDqjnh"
      },
      "source": [
        "importances = rf_random.best_estimator_.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in rf_random.best_estimator_],\n",
        "             axis=0)\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "# Plot the feature importances of the forest\n",
        "plt.figure()\n",
        "plt.title(\"Feature importances\")\n",
        "plt.barh(range(X.shape[1]), importances[indices],\n",
        "       color=\"b\", xerr=std[indices], align=\"center\")\n",
        "# If you want to define your own labels,\n",
        "# change indices to a list of labels on the following line.\n",
        "plt.yticks(range(X.shape[1]), indices)\n",
        "plt.ylim([-1, X.shape[1]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIrHRb5zqmHc"
      },
      "source": [
        "feature_importance_labels=df_merge3.drop(columns=['lukashenko_share','Dmitriev', 'Kanopatskaya', 'Lukashenko', 'Tikhanovskaya', 'Cherechen'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vksgHsttqoEk"
      },
      "source": [
        "column_names=list(feature_importance_labels.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIPsLwWEqqEQ"
      },
      "source": [
        "column_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ILaIdANqsD0"
      },
      "source": [
        "values_imp=list(importance_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqU9B2omqt_z"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        " \n",
        "courses = ['total','receiv','tookpart','earlvoting','residence','electday','dur_earl_vot','againts_all','held_on','invalid','rec_comm','spoiled','unusedb','latitude','longitude','doctor','army','education','science','prof_union','economics','brest','gomel', 'grod','minsk','miskaya','mogilev','no_region','vitebsk']\n",
        "values = values_imp\n",
        "\n",
        "plt.bar(courses, values, color ='blue')\n",
        "\n",
        "plt.xticks(rotation = 60)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFBGdDL-qv5e"
      },
      "source": [
        "df_merge3.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TAk4vCMn4b5"
      },
      "source": [
        "\n",
        "# Feature Selection, Regression Analysis and Exploratory Data Analysis in R\n",
        "We undertake the regression models in R. \n",
        "The following sections are structured as follows: \n",
        "\n",
        "1.   Lasso Regression compared to a naive model \n",
        "2.   Regression model with the selected features for LASSO\n",
        "3.   Regression model with the selected features for the random forest \n",
        "4.   Feature importance in lasso regression compared to the random forest.  \n",
        "5.   Exploration of unsupervised learning methods Part 1: Principal Component Analysis \n",
        "6.   Exploration of unsupervised learning methods Part 2: Latent Dirichlet Allocation \n",
        "7.   Geospatial Analysis of Voting Behaviour \n",
        "\n",
        "Prior to including R code, we need to install 'rpy2' package.\n",
        "Further, every cell needs to start with %%R for the Markdown file to differentiate between R code and Python code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L--2hJ7TrLgB"
      },
      "source": [
        "!pip install rpy2\n",
        "%load_ext rpy2.ipython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y2pas--pLoR"
      },
      "source": [
        "## LASSO Regression for Feature Selection\n",
        "\n",
        "In the first step we need to install our packages and set our seed for replicability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY--UkZgEsdO"
      },
      "source": [
        "%%R\n",
        "\n",
        "install.packages(\"skimr\")\n",
        "install.packages(\"glmnet\")\n",
        "install.packages(\"plotmo\")\n",
        "\n",
        "library(readxl)\n",
        "library(MASS)\n",
        "library(tidyverse)\n",
        "library(broom)\n",
        "library(skimr)\n",
        "library(glmnet)\n",
        "library(plotmo)\n",
        "set.seed(144)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtR4GfNen8No"
      },
      "source": [
        "## Splitting the data \n",
        "\n",
        "In the first step, we split the data into training and test set. \n",
        "We use a split of 0.66.\n",
        "For reference, we are using a slightly modified dataset here because we use regression as a categorical variable and not use dummies for each region due to the limited number of observations which prove to be inconvient for subsequent regression analysis. Irrespective, we will later on look at geospatial voting information to get a more fine grained picture of what the geospatial variation looks like. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5w26y6YFTAX"
      },
      "source": [
        "%%R\n",
        "set.seed(123)\n",
        "data <- read_excel(\"df_merge_d.xlsx\")\n",
        "data = data[-(9:13)] # removing \n",
        "p = ncol(data) -1\n",
        "n <- nrow(data)\n",
        "ind_train = sample(x = 1:n, size = ceiling(0.66 * n))\n",
        "set_train = data[ind_train,]\n",
        "ind_test = setdiff(x=1:n, ind_train)\n",
        "set_test = data[ind_test,] \n",
        "head(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOEM4Ou5qNqc"
      },
      "source": [
        "Now we can also look at some summary statistics.\n",
        "Optionally, we can also look at summary statistics itself using the summary command, but some recent data exploration packages allow for a more interesting look at different variables also plotting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DPy8I0AF-K_"
      },
      "source": [
        "%%R\n",
        "summary(set_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CXVyRqMGsuY"
      },
      "source": [
        "%%R\n",
        "par(mfrow = c(3, 1))\n",
        "hist(data$lukashenko_share) \n",
        "hist(set_train$lukashenko_share)\n",
        "hist(set_test$lukashenko_share)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hItJx9RzGr7q"
      },
      "source": [
        "%%R\n",
        "skim(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11i4vpiV7ev6"
      },
      "source": [
        "## Correlation Plot\n",
        "\n",
        "Next we can also look at the correlation plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU95wOGJiG9o"
      },
      "source": [
        "%%R\n",
        "install.packages(\"corrplot\")\n",
        "library(corrplot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUgKAz6iuW_F"
      },
      "source": [
        "%%R\n",
        "correl = cor(data[-(1:2)])\n",
        "corrplot(correl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpTtiFrIiYxK"
      },
      "source": [
        "%%R\n",
        "model_lasso = glmnet(x = as.matrix(set_train[, - (p+1)]), y = set_train$lukashenko_share, alpha =1)\n",
        "\n",
        "cv<-cv.glmnet(as.matrix(set_train[-(p+1)]), set_train$lukashenko_share, nfolds = 3) \n",
        "plot(cv)\n",
        "\n",
        "lambda_lasso = cv.glmnet(x = as.matrix(set_train[,-(p+1)]),y = set_train$lukashenko_share, alpha =1)$lambda.min\n",
        "\n",
        "# Plot log lambda\n",
        "plot_glmnet(x = model_lasso, label = TRUE, xvar = \"lambda\")\n",
        "title(main = \"LASSO\", line = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbbUoByZidaG"
      },
      "source": [
        "\n",
        "We can first look at how our models perform on our training data.\n",
        "Here we are comparing a random forest and a lasso model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "522cBvbvictY"
      },
      "source": [
        "%%R\n",
        "y_train = set_train$lukashenko_share\n",
        "predict_train = matrix(data =0, nrow= nrow(set_train), ncol=2)\n",
        "\n",
        "predict_train[, 1] = predict(object = model_naive, newdata = set_train[, -(p + 1)])\n",
        "predict_train[,2] = predict.glmnet(object = model_lasso, \n",
        "                                  newx = as.matrix(set_train[,-(p+1)]),\n",
        "                                  s = lambda_lasso)\n",
        "\n",
        "colnames(predict_train) =c(\"Naive Model\", \"Lasso Model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haaMOadSihY4"
      },
      "source": [
        "Predicting on our training data, we can see that the OLS model without regularization performs better than the Lasso model, although they are somewhat similar. \n",
        "Subsequently, we can look at how our model performs when dealing with new data. \n",
        "Therefore, we use the test set and again predict our naive and our lasso model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdeOw9KJif1U"
      },
      "source": [
        "%%R\n",
        "y_test = set_test$lukashenko_share\n",
        "predict_test = matrix(data =0, nrow= nrow(set_test), ncol=2)\n",
        "\n",
        "predict_test[, 1] = predict(object = model_naive, newdata = set_test[, -(p + 1)])\n",
        "predict_test[,2] = predict.glmnet(object = model_lasso,  newx = as.matrix(set_test[,-(p+1)]),\n",
        "                                  s = lambda_lasso)\n",
        "\n",
        "colnames(predict_test) =c(\"Naive Model\", \"Lasso Model\")\n",
        "\n",
        "MSE_test = rep(x=0, length.out =2) \n",
        "for (i in 1:2){\n",
        "  MSE_test[i] = mean((y_test-predict_test[,i])^2)\n",
        "}\n",
        "names(MSE_test) = c(\"Naive Model\", \"Lasso Model\")\n",
        "MSE_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5FfAi8usFKo"
      },
      "source": [
        "# Feature Importance for Lasso Regression \n",
        " \n",
        " For comparability, we also include "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkKoHnXvsGsU"
      },
      "source": [
        "%%R\n",
        "set.seed(123)\n",
        "install.packages(\"ranger\")\n",
        "install.packages(\"vip\")\n",
        "library(ranger)\n",
        "library(vip) # Link: https://koalaverse.github.io/vip/articles/vip.html\n",
        "rfo <- ranger(lukashenko_share ~ ., data = data, importance = \"permutation\")\n",
        "rfo\n",
        "vi_rfo <- rfo$variable.importance\n",
        "vi_rfo\n",
        "barplot(vi_rfo, horiz = TRUE, las = 1)\n",
        "\n",
        "vip(rfo, width = 0.5, aesthetics = list(fill = \"green3\"))\n",
        "# backward <- step(model_lasso, direction = \"backward\", trace = 0)\n",
        "#vip(model_lasso, width = 0.5, aesthetics = list(fill = \"green3\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j_ASlA4ilWE"
      },
      "source": [
        "# OLS Regression with selected features\n",
        "\n",
        "Here we can see a very different performance. \n",
        "The Lasso model performs substantially better then the OLS model.\n",
        "Therefore, using this method has led to better predictive performance. \n",
        "Lastly, we can also use the lasso specification for a OLS model, trained on test data to ensure that we have no incorrect standard errors.\n",
        "This is a pivotal concern within in economics, because we need correct standard errors to interpret our coefficients and ensure causality. \n",
        "\n",
        "As a first step we can look at the coefficients in the lasso model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnqUSq9giqXP"
      },
      "source": [
        "coef_lasso <- model_lasso$beta[, which(model_lasso$lambda == lambda_lasso)]\n",
        "which(coef_lasso!=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxgUXBiNiuPv"
      },
      "source": [
        "The advantage of the lasso method is that we induce sparsity.\n",
        "Here we can see that various features turn to 0.\n",
        "One could also argue that due to correlation between some features, explanatory or variance was ascribed to either the one or the other variable. \n",
        "We can see that the variables that were more correlated (see correlation plot above) frequently scored 0 here. \n",
        "Now we can specify a model with the relevant coefficients, and run the model on the test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR8juB3LivSI"
      },
      "source": [
        "model = lm(lukashenko_share ~  earlyvoting  + residence + electday + dropped + against_all + commission + spoiled +  long + army + educ + science + profunion + econ, data = set_test)\n",
        "summary(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zscDvlKi9Ae"
      },
      "source": [
        "# Alternative Methods: Unsupervised Learning via Principal Component Analysis \n",
        "\n",
        "In addition to supervised learning, we can also look at feature importance from the perspective of unsupervised learning.\n",
        "One popular dimensionality reduction technique is principal component analysis or in short PCA. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXwEtLvpi2hE"
      },
      "source": [
        "%%R\n",
        "# install.packages(\"ggfortify\")\n",
        "library(ggfortify)\n",
        "library(devtools)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxTfJ8cWzmMV"
      },
      "source": [
        "%%R\n",
        "data_n = data[-2]\n",
        "PCA = prcomp(data_n, scale. = TRUE,center = TRUE)\n",
        "summary(PCA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oCgV9AB0FDd"
      },
      "source": [
        "%%R\n",
        "screeplot(PCA, npcs =8, type = \"lines\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIC3QoN30grO"
      },
      "source": [
        "Optional: Further Analysis \n",
        "\n",
        "Future analysis could also use further graphical tools to examine variation in subgroups. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iUru0qz0ets"
      },
      "source": [
        "%%R\n",
        "# install_github(\"vqv/ggbiplot\")\n",
        "# library(ggbiplot)\n",
        "ggbiplot(PCA,ellipse=TRUE, groups= data$education)\n",
        "#autoplot(PCA, colour = \"lukashenko_share\", loadings =T,loading.label =T,loadings.label.size = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs-orD9ejbxK"
      },
      "source": [
        "# Alternative Methods: Unsupervised Learning via Latent Dirichlet Allocation \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob8FW2Rt1j-4"
      },
      "source": [
        "%%R\n",
        "\n",
        "install.packages(\"topicmodels\")\n",
        "#install.packages(\"tidytext\")\n",
        "#install.packages(\"lda\")\n",
        "#install.packages(\"tm\")\n",
        "\n",
        "library(tidyverse)\n",
        "library(tidytext)\n",
        "library(topicmodels)\n",
        "library(lda)\n",
        "library(tm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIYrETrE4BrQ"
      },
      "source": [
        "%%R\n",
        "install.packages(\"githubinstall\")\n",
        "library(githubinstall)\n",
        "githubinstall(\"topicmodels\")\n",
        "#library(tm)\n",
        "library(topicmodels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKmbPTh26ax1"
      },
      "source": [
        "%%R\n",
        "install.packages(\"lda\")\n",
        "library(lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMW-f0OkjSSl"
      },
      "source": [
        "%%R\n",
        "data = read_excel(\"/df_merge_n.xlsx\")\n",
        "data = unite(data, col = \"text\", commission_code:economics, sep =\" \")\n",
        "summary(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXtMIT1akVvu"
      },
      "source": [
        "## Cleaning the Text data \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oobt4iWhj49Z"
      },
      "source": [
        "%%R\n",
        "data = data %>%\n",
        "  select(\"text\")\n",
        "\n",
        "cleaner <- function(text){\n",
        "  text <- tolower(text)\n",
        "  text <- gsub(\"rt\", \"\", text)\n",
        "  text <- gsub(\"@\\\\w+\", \"\", text)\n",
        "  text <- gsub(\"[[:punct:]]\", \"\", text)\n",
        "  text <- gsub(\"http\\\\w+\", \"\", text)\n",
        "  text <- gsub(\"amp\", \" \", text)\n",
        "  text <- gsub(\"[ |\\t]{2,}\", \"\", text)\n",
        "  text <- gsub(\"^ \", \"\", text)\n",
        "  text <- gsub(\" $\", \"\", text)\n",
        "  text <- gsub(\" +\", \" \", text)\n",
        "  text <- gsub(\"=\", \" \", text)\n",
        "  text <- gsub('<.*>', '', enc2native(text))\n",
        "  text <- unique(text)\n",
        "  return(text)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvcyy-hO7rG_"
      },
      "source": [
        "%%R\n",
        "polish <- function(text){\n",
        "  text <- VCorpus(VectorSource(text))\n",
        "  text <- tm_map(text, removeWords, stopwords(\"russian\")) # used to be english, still WIP\n",
        "  text <- tm_map(text, removeNumbers)\n",
        "  text <- tm_map(text, stemDocument)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56ie55d_j9qz"
      },
      "source": [
        "%%R\n",
        "text = data$text \n",
        "text <- cleaner(text)\n",
        "corpus <- polish(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um8ybhn_7tEa"
      },
      "source": [
        "%%R\n",
        "doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(corpus)))\n",
        "dtm <- DocumentTermMatrix(corpus[doc.lengths > 0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ny2emoQ8Z2d"
      },
      "source": [
        "## Setting up our Topic Model using VEM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6_A8QDcj7gt"
      },
      "source": [
        "%%R\n",
        "\n",
        "LDA_V <- LDA(dtm, k =3,method = \"VEM\", cotrol = list(seed=1234))\n",
        "topics_1 <- tidy(LDA_V, matrix =\"beta\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HknMXNk8k9b"
      },
      "source": [
        "Next we can order the terms and select only the top 10 terms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nZZ1fUG7v7h"
      },
      "source": [
        "%%R\n",
        "ap_top_terms <- topics_1 %>%\n",
        "  group_by(topic) %>%\n",
        "  top_n(10, beta) %>%\n",
        "  ungroup() %>%\n",
        "  arrange(topic, -beta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL2IRSEk8ltY"
      },
      "source": [
        "Now we can plot these terms using ggplot: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYpyb07_77cQ"
      },
      "source": [
        "%%R\n",
        "ap_top_terms %>%\n",
        "  mutate(term = reorder_within(term, beta, topic)) %>%\n",
        "  ggplot(aes(term, beta, fill = factor(topic))) +\n",
        "  geom_col(show.legend = FALSE) +\n",
        "  facet_wrap(~ topic, scales = \"free\") +\n",
        "  coord_flip() +\n",
        "  scale_x_reordered()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCclaQDi8KSI"
      },
      "source": [
        "## Topic Model with Gibbs sampling \n",
        "\n",
        "We can also modify our topic model by using gibbs sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP0iCgdJ7-6k"
      },
      "source": [
        "%%R\n",
        "LDA_G <- LDA(x=dtm, k=2, method=\"Gibbs\",control=list(alpha=1, delta=0.1, seed=10005))\n",
        "topics_2 <- tidy(LDA_V, matrix =\"beta\")\n",
        "\n",
        "ap_top_terms <- topics_2 %>%\n",
        "  group_by(topic) %>%\n",
        "  top_n(10, beta) %>%\n",
        "  ungroup() %>%\n",
        "  arrange(topic, -beta)\n",
        "\n",
        "  ap_top_terms %>%\n",
        "  mutate(term = reorder_within(term, beta, topic)) %>%\n",
        "  ggplot(aes(term, beta, fill = factor(topic))) +\n",
        "  geom_col(show.legend = FALSE) +\n",
        "  facet_wrap(~ topic, scales = \"free\") +\n",
        "  coord_flip() +\n",
        "  scale_x_reordered()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8aHAJwvkRv-"
      },
      "source": [
        "# Geospatial Analysis (#TODO)\n",
        "\n",
        "We cal also plot the votes by area. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed8fPx93kREU"
      },
      "source": [
        "%%R\n",
        "library(ggplot2)\n",
        "library(sf)\n",
        "library(maps)\n",
        "\n",
        "geo = st_read(\"~/Downloads/BelarusElections 4/gis files for BLR and poland/BLR_Adm/BLR_adm2.shp\")\n",
        "dist = st_read(\"~/Downloads/BelarusElections 4/gis files for BLR and poland/BLR_Adm/BLR_adm1.shp\")\n",
        "road = st_read(\"~/Downloads/BelarusElections 4/gis files for BLR and poland/BLR_rds/BLR_roads.shp\")\n",
        "data <- read_excel(\"df_merge_d.xlsx\", sheet = \"Sheet1\")\n",
        "\n",
        "merge(geo, data, by.x= region, by.y=region)\n",
        "\n",
        "plot = ggplot()+ geom_sf(data = geo, mapping = aes(geometry = geometry)) + \n",
        "  geom_sf(data= dist, aes(geometry = geometry)) + \n",
        "  geom_sf(data= road, aes(geometry = geometry)) \n",
        "\n",
        "map = plot + geom_point(data = bela, mapping = aes(fill = \"region\")) + coord_sf()\n",
        "map\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}