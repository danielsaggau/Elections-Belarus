{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Belarus ML Full Analysis.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOc4aEj0k1hUDDjSO6+v4HJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielsaggau/Elections-Belarus/blob/main/Belarus_ML_Full_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHEUISpniVV1"
      },
      "source": [
        "!pip install rpy2\n",
        "%load_ext rpy2.ipython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOuLBwofEYYc"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df_pls=pd.read_excel(\"df_merge_n.xlsx\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBryEZuZHKQb"
      },
      "source": [
        "df_pls_pls=df_pls.drop(columns=['Unnamed: 0'])\n",
        "df_pls_pls=df_pls_pls.fillna(0)\n",
        "df_pls_new=df_pls_pls.drop(columns=['commission_code','number_of_cite'])\n",
        "df_pls_new.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnEN3jXtHT5r"
      },
      "source": [
        "df_merge1=df_pls_new.drop(columns=['attachment1','attachment2','attachment3','attachment4','attachment5','comment','id','type','name','location','origin','area'])\n",
        "df_merge1.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuUEJ6mqHgLG"
      },
      "source": [
        "df_merge1['lukashenko_share']=df_merge1['Lukashenko']/df_merge1['number_of_voters_who_took_part_in_the_voting']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acawmg8fHiZV"
      },
      "source": [
        "df_merge1['lukashenko_share']=df_merge1['lukashenko_share'].replace([np.inf, -np.inf], 0)\n",
        "df_merge1.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWA9G7OXHnih"
      },
      "source": [
        "df_merge2=df_merge1.drop(columns=['parent_id','description_x','commission_id','work_title','description_y','city_or_district_within_the_region','area_in_the_city','settlement'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkENN5lDHqSw"
      },
      "source": [
        "df_merge2.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp9NQuOxHsiF"
      },
      "source": [
        "df_merge2['region'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eho6rxJaIw2A"
      },
      "source": [
        "df_merge2['region']=df_merge2['region'].replace('Брестская','brestskaya')\n",
        "df_merge2['region']=df_merge2['region'].replace('Витебская','vitebskaya')\n",
        "df_merge2['region']=df_merge2['region'].replace('Гомельская','gomelskaya')\n",
        "df_merge2['region']=df_merge2['region'].replace('Гродненская','grodnenskaya')\n",
        "df_merge2['region']=df_merge2['region'].replace('Минская','miskaya')\n",
        "df_merge2['region']=df_merge2['region'].replace('Могилевская','mogilevskaya')\n",
        "df_merge2['region']=df_merge2['region'].replace('город Минск','minsk')\n",
        "df_merge2.region=df_merge2.region.replace(0, 'no_region')\n",
        "# df_merge3=pd.get_dummies(df_merge2, 'region')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLBsMMaRIzxG"
      },
      "source": [
        "df_merge2.info()\n",
        "df_merge_d=df_merge2.fillna(0)\n",
        "df_merge_d.to_excel(\"df_merge_d.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfwwUmz6Kkz9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold \n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RR0M77aQwio"
      },
      "source": [
        "\n",
        "# обязательный код\n",
        "X=df_merge_d.drop(columns=['lukashenko_share','Dmitriev','Kanopatskaya','Lukashenko','Tikhanovskaya','Cherechen']).values\n",
        "y=df_merge_d.lukashenko_share.values\n",
        "# сплитуйте X и y как хотите\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, \n",
        "                                                    shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UrHRNrSQ2D3"
      },
      "source": [
        "rf = RandomForestRegressor(random_state = 42)\n",
        "from pprint import pprint\n",
        "# Look at parameters used by our current forest\n",
        "print('Parameters currently in use:\\n')\n",
        "pprint(rf.get_params())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpsSI3nuQ7wa"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1600, num = 8)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 90, num = 9)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "pprint(random_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTQFhSVgQ-uM"
      },
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "rf = RandomForestRegressor()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2KHun2RiF5a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8wrdyOhET4q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY--UkZgEsdO"
      },
      "source": [
        "%%R\n",
        "\n",
        "install.packages(\"skimr\")\n",
        "install.packages(\"glmnet\")\n",
        "install.packages(\"plotmo\")\n",
        "\n",
        "library(readxl)\n",
        "library(MASS)\n",
        "library(tidyverse)\n",
        "library(broom)\n",
        "library(skimr)\n",
        "library(glmnet)\n",
        "library(plotmo)\n",
        "set.seed(144)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5w26y6YFTAX"
      },
      "source": [
        "%%R\n",
        "data <- read_excel(\"df_merge_d.xlsx\", sheet = \"Sheet1\")\n",
        "\n",
        "p = ncol(data) -1\n",
        "n <- nrow(data)\n",
        "colnames(data) = c(\"total\", \"\")\n",
        "ind_train = sample(x = 1:n, size = ceiling(0.66 * n))\n",
        "set_train = data[ind_train,]\n",
        "ind_test = setdiff(x=1:n, ind_train)\n",
        "set_test = data[ind_test,] \n",
        "head(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DPy8I0AF-K_"
      },
      "source": [
        "%%R\n",
        "summary(set_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CXVyRqMGsuY"
      },
      "source": [
        "%%R\n",
        "par(mfrow = c(3, 1))\n",
        "hist(data$lukashenko_share) \n",
        "hist(set_train$lukashenko_share)\n",
        "hist(set_test$lukashenko_share)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hItJx9RzGr7q"
      },
      "source": [
        "%%R\n",
        "skim(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU95wOGJiG9o"
      },
      "source": [
        "%%R\n",
        "install.packages(\"corrplot\")\n",
        "library(corrplot)\n",
        "correl = cor(data[-1])\n",
        "corrplot(correl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpTtiFrIiYxK"
      },
      "source": [
        "model_lasso = glmnet(x = as.matrix(set_train[, - (p+1)]), y = set_train$lukashenko_share, alpha =1)\n",
        "\n",
        "cv<-cv.glmnet(as.matrix(set_train[-(p+1)]), set_train$lukashenko_share, nfolds = 3) \n",
        "plot(cv)\n",
        "\n",
        "lambda_lasso = cv.glmnet(x = as.matrix(set_train[,-(p+1)]),y = set_train$lukashenko_share, alpha =1)$lambda.min\n",
        "\n",
        "# Plot log lambda\n",
        "plot_glmnet(x = model_lasso, label = TRUE, xvar = \"lambda\")\n",
        "title(main = \"LASSO\", line = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbbUoByZidaG"
      },
      "source": [
        "\n",
        "We can first look at how our models perform on our training data.\n",
        "Here we are comparing a random forest and a lasso model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "522cBvbvictY"
      },
      "source": [
        "y_train = set_train$lukashenko_share\n",
        "predict_train = matrix(data =0, nrow= nrow(set_train), ncol=2)\n",
        "\n",
        "predict_train[, 1] = predict(object = model_naive, newdata = set_train[, -(p + 1)])\n",
        "predict_train[,2] = predict.glmnet(object = model_lasso, \n",
        "                                  newx = as.matrix(set_train[,-(p+1)]),\n",
        "                                  s = lambda_lasso)\n",
        "\n",
        "colnames(predict_train) =c(\"Naive Model\", \"Lasso Model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haaMOadSihY4"
      },
      "source": [
        "Predicting on our training data, we can see that the OLS model without regularization performs better than the Lasso model, although they are somewhat similar. \n",
        "Subsequently, we can look at how our model performs when dealing with new data. \n",
        "Therefore, we use the test set and again predict our naive and our lasso model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdeOw9KJif1U"
      },
      "source": [
        "y_test = set_test$lukashenko_share\n",
        "predict_test = matrix(data =0, nrow= nrow(set_test), ncol=2)\n",
        "\n",
        "predict_test[, 1] = predict(object = model_naive, newdata = set_test[, -(p + 1)])\n",
        "predict_test[,2] = predict.glmnet(object = model_lasso,  newx = as.matrix(set_test[,-(p+1)]),\n",
        "                                  s = lambda_lasso)\n",
        "\n",
        "colnames(predict_test) =c(\"Naive Model\", \"Lasso Model\")\n",
        "\n",
        "MSE_test = rep(x=0, length.out =2) \n",
        "for (i in 1:2){\n",
        "  MSE_test[i] = mean((y_test-predict_test[,i])^2)\n",
        "}\n",
        "names(MSE_test) = c(\"Naive Model\", \"Lasso Model\")\n",
        "MSE_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j_ASlA4ilWE"
      },
      "source": [
        "# OLS Regression with selected features\n",
        "\n",
        "Here we can see a very different performance. \n",
        "The Lasso model performs substantially better then the OLS model.\n",
        "Therefore, using this method has led to better predictive performance. \n",
        "Lastly, we can also use the lasso specification for a OLS model, trained on test data to ensure that we have no incorrect standard errors.\n",
        "This is a pivotal concern within in economics, because we need correct standard errors to interpret our coefficients and ensure causality. \n",
        "\n",
        "As a first step we can look at the coefficients in the lasso model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnqUSq9giqXP"
      },
      "source": [
        "coef_lasso <- model_lasso$beta[, which(model_lasso$lambda == lambda_lasso)]\n",
        "which(coef_lasso!=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxgUXBiNiuPv"
      },
      "source": [
        "The advantage of the lasso method is that we induce sparsity.\n",
        "Here we can see that various features turn to 0.\n",
        "One could also argue that due to correlation between some features, explanatory or variance was ascribed to either the one or the other variable. \n",
        "We can see that the variables that were more correlated (see correlation plot above) frequently scored 0 here. \n",
        "Now we can specify a model with the relevant coefficients, and run the model on the test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR8juB3LivSI"
      },
      "source": [
        "model = lm(lukashenko_share ~  earlyvoting  + residence + electday + dropped + against_all + commission + spoiled +  long + army + educ + science + profunion + econ, data = set_test)\n",
        "summary(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM7dAvwCkA2b"
      },
      "source": [
        "# Feature Importance for Lasso Regression \n",
        " \n",
        " For comparability, we also include "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zscDvlKi9Ae"
      },
      "source": [
        "# Alternative Methods: Unsupervised Learning via Principal Component Analysis \n",
        "\n",
        "In addition to supervised learning, we can also look at feature importance from the perspective of unsupervised learning.\n",
        "One popular dimensionality reduction technique is principal component analysis or in short PCA. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXwEtLvpi2hE"
      },
      "source": [
        "library(ggfortify)\n",
        "library(devtools)\n",
        "install_github(\"vqv/ggbiplot\")\n",
        "library(ggbiplot)\n",
        "\n",
        "summary(PCA)\n",
        "screeplot(PCA, npcs =8, type = \"lines\")\n",
        "install.packages(\"ggfortify\")\n",
        "install.packages(\"\")\n",
        "autoplot(PCA, colour = \"lukashenko_share\", loadings =T,loading.label =T,loadings.label.size = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5aDCr4Bi57j"
      },
      "source": [
        "PCA = prcomp(data[,19:31], scale. = TRUE,center = TRUE)\n",
        "summary(PCA)\n",
        "ggbiplot(PCA,ellipse=TRUE, labels = rownames(data), groups= data$regions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbV4rYYzjRWJ"
      },
      "source": [
        "# Interpretation \n",
        "\n",
        "Here one can see (#TODO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs-orD9ejbxK"
      },
      "source": [
        "# Alternative Methods: Unsupervised Learning via Latent Dirichlet Allocation \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMW-f0OkjSSl"
      },
      "source": [
        "%%R\n",
        "install.packages(\"lda\")\n",
        "install.packages(\"topicmodels\")\n",
        "install.packages(\"tm\")\n",
        "\n",
        "library(tidyverse)\n",
        "library(lda)\n",
        "library(topicmodels)\n",
        "library(tidytext)\n",
        "library(tm)\n",
        "\n",
        "#data = unite(lits3_gis,col = \"text\", country:b2c, sep =\" \")\n",
        "#data <- read_csv(\"~/Downloads/BelarusElections/belarus-qgis.csv\")\n",
        "data <- read_excel(\"df_merge_n.xlsx\", sheet = \"Sheet1\")\n",
        "data = unite(data, col = \"text\", commission_code:economics, sep =\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXtMIT1akVvu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oobt4iWhj49Z"
      },
      "source": [
        "data = data %>%\n",
        "  select(\"text\")\n",
        "\n",
        "cleaner <- function(text){\n",
        "  text <- tolower(text)\n",
        "  text <- gsub(\"rt\", \"\", text)\n",
        "  text <- gsub(\"@\\\\w+\", \"\", text)\n",
        "  text <- gsub(\"[[:punct:]]\", \"\", text)\n",
        "  text <- gsub(\"http\\\\w+\", \"\", text)\n",
        "  text <- gsub(\"amp\", \" \", text)\n",
        "  text <- gsub(\"[ |\\t]{2,}\", \"\", text)\n",
        "  text <- gsub(\"^ \", \"\", text)\n",
        "  text <- gsub(\" $\", \"\", text)\n",
        "  text <- gsub(\" +\", \" \", text)\n",
        "  text <- gsub(\"=\", \" \", text)\n",
        "  text <- gsub('<.*>', '', enc2native(text))\n",
        "  text <- unique(text)\n",
        "  return(text)\n",
        "}\n",
        "#####################################################################################\n",
        "\n",
        "polish <- function(text){\n",
        "  text <- VCorpus(VectorSource(text))\n",
        "  text <- tm_map(text, removeWords, stopwords(\"english\"))\n",
        "  text <- tm_map(text, removeNumbers)\n",
        "  text <- tm_map(text, stemDocument)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56ie55d_j9qz"
      },
      "source": [
        "text = data$text \n",
        "text <- cleaner(text)\n",
        "corpus <- polish(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6_A8QDcj7gt"
      },
      "source": [
        "doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(corpus)))\n",
        "dtm <- DocumentTermMatrix(corpus[doc.lengths > 0])\n",
        "\n",
        "#####################################################################################\n",
        "\n",
        "#LDA <- LDA(dtm, k =3,method = \"VEM\", cotrol = list(seed=1234))\n",
        "LDA <- LDA(x=dtm, k=2, method=\"Gibbs\",control=list(alpha=1, delta=0.1, seed=10005))\n",
        "\n",
        "topics <- tidy(LDA, matrix =\"beta\")\n",
        "\n",
        "ap_top_terms <- topics %>%\n",
        "  group_by(topic) %>%\n",
        "  top_n(10, beta) %>%\n",
        "  ungroup() %>%\n",
        "  arrange(topic, -beta)\n",
        "\n",
        "#####################################################################################\n",
        "\n",
        "ap_top_terms %>%\n",
        "  mutate(term = reorder_within(term, beta, topic)) %>%\n",
        "  ggplot(aes(term, beta, fill = factor(topic))) +\n",
        "  geom_col(show.legend = FALSE) +\n",
        "  facet_wrap(~ topic, scales = \"free\") +\n",
        "  coord_flip() +\n",
        "  scale_x_reordered()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8aHAJwvkRv-"
      },
      "source": [
        "# Geospatial Analysis "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed8fPx93kREU"
      },
      "source": [
        "%%R\n",
        "library(ggplot2)\n",
        "library(sf)\n",
        "library(maps)\n",
        "\n",
        "votes <- read.csv(\"~/Downloads/BelarusElections/belarus1994-qgisNew.csv\",header=T)\n",
        "geo = st_read(\"~/Downloads/BelarusElections 4/gis files for BLR and poland/BLR_Adm/BLR_adm2.shp\")\n",
        "dist = st_read(\"~/Downloads/BelarusElections 4/gis files for BLR and poland/BLR_Adm/BLR_adm1.shp\")\n",
        "road = st_read(\"~/Downloads/BelarusElections 4/gis files for BLR and poland/BLR_rds/BLR_roads.shp\")\n",
        "bela <- read.csv(\"~/Downloads/BelarusElections 3/belarus-qgis.csv\", header=T)\n",
        "data <- read_excel(\"df_merge_n.xlsx\", sheet = \"Sheet1\")\n",
        "\n",
        "plot = ggplot()+ geom_sf(data = geo, mapping = aes(geometry = geometry)) + \n",
        "  geom_sf(data= dist, aes(geometry = geometry)) + \n",
        "  geom_sf(data= road, aes(geometry = geometry)) \n",
        "\n",
        "map = plot + geom_point(data = bela, mapping = aes(fill = \"region\")) + coord_sf()\n",
        "map\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}