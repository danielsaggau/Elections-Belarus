---
title: "code_elections"
author: ""
date: "3/7/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Installing the packages 


```{r}
library(readxl)
library(MASS)
library(glmnet)
library(tidyverse)
library(broom)
library(glmnet)
```


# LASSO Regression for Feature Selection

# Splitting the data 

In the first step, we split the data into training and test set. 
We use a split of 0.66.

```{r}
# We set a seed for reproducability
set.seed(123)
library(tidyverse)
library(skimr)
library(mlr3)
library(glmnet)

# Importing the dataset
data <- read.csv("~/Elections-Belarus/train_pls1")

# Remove inf
data = data %>% 
  filter_all(all_vars(!is.infinite(.)))

colnames(data) = c("X","total", "received", "tookpart","earlyvoting", "residence", "electday", "dmitriev", "kano", "lukashenko","tik","cher", "dropped","against_all", "held_on", "invalid","commission","spoiled","unused","lat","long","doc","army","educ", "science","profunion","econ","lukashenko_share")

# Remove columns we dont need 
data = data[-(8:12)]
n <- nrow(data)
# Split the data
ind_train = sample(x = 1:n, size = ceiling(0.8 * n))
set_train = data[ind_train,]
ind_test = setdiff(x=1:n, ind_train)
set_test = data[ind_test,] 
```

# Relabeling and cleaning up  

```{r}

```

# Brief Summary statistics and exploration

```{r}


model = lm(lukashenko_share ~ total + tookpart + received + earlyvoting + residence + electday + against_all + held_on + invalid + commission + spoiled + unused + profunion + science + army +educ + lat + long, data = data)

summary(model)

hist(data$lukashenko_share)
summary(data$lukashenko_share)
summary(set_train$lukashenko_share)
summary(set_test$lukashenko_share)
cor(set_train)
skim(data)

distplot(data$lukashenko_share)
```


# Cross Validation 

```{r}
# Number of observations in row
n_train <- nrow(set_train)

# Remove missing and inf
data = data %>% 
  filter_all(all_vars(!is.infinite(.)))
p = ncol(data) -1

# Sequence for lambda :
lambda <- seq(from = 0.01, to = 1, by = 0.01)

# instantiate cv criteria 
cv_lasso <- rep(x = 0, length.out = length(lambda))

#Predict lasso 
predict_lasso <- predict.glmnet(object = glmnet(as.matrix(set_train_cv[, -(p + 1)]), set_train_cv$lukashenko_share, alpha = 1),
newx = as.matrix(set_test_cv[, -(p + 1)]),
s = lambda[i])
# Berechnung der Fehlerquadratsummen (SSE):
cv_lasso[i] <- cv_lasso[i] + sum((set_test_cv$ - predict_lasso)^2)
# Berechnung der MSEs:
cv_lasso <- cv_lasso / n_train
```


```{r}
# Lambda score via cross validation and normal lambda score
lambda_lasso_cv <- lambda[which(cv_lasso == min(cv_lasso))]
lambda_lasso_cv
lambda_lasso
```



```{r}
model_lasso = glmnet(x = as.matrix(data[, - (p+1)]), y = data$lukashenko_share, alpha =1)
plot(model_lasso)
```


```{r}
lambda_lasso = cv.glmnet(x = as.matrix(data[,-(p+1)]),
                         y = data$lukashenko_share, alpha =1)$lambda.min

coef_lasso <- model_lasso$beta[, which(model_lasso$lambda == lambda_lasso)]
coef_lasso

# LASSO-Regularisierung:
library(plotmo)
par(mfrow = c(1, 1))
# LASSO-Regularisierung:
plot_glmnet(x = model_lasso, label = TRUE, xvar = "lambda")
title(main = "LASSO", line = 3)
```

```{r}
ind = which(coef_lasso!=0)
ind
```

# OLS Regression with selected features

```{r}
model = lm(lukashenko_share ~ number_of_voters_who_took_part_in_voting_at_the_place_of_residence  + Dmitriev + Lukashenko + Tikhanovskaya +number_of_votes_againts_all_candidates + latitude + longitude, data = data)

summary(model)

```

# RF Model (#TODO)

```{r}
n = nrow(data)
p = ncol(data) -1

cor(data)
model_lasso = glmnet(x = as.matrix(data[, - (p+1)]), y = data$lukashenko_share, alpha =1)

lambda_lasso = cv.glmnet(x = as.matrix(data[,-(p+1)]),
                         y = data$total, alpha =1)$lambda.min

coef_lasso <- model_lasso$beta[, which(model_lasso$lambda == lambda_lasso)]
coef_lasso
```


# Mergning Data with the Life in Transition Survey (#TOD0)

```{r}

```


# Geospatial Map  (#TOD0)

```{r}
library(ggplot2)
bela <- read.csv("~/Downloads/BelarusElections 3/belarus-qgis.csv", header=T)
ggplot(bela, aes (x = latitude, y= longitude)) + geom_point()


```


# Preprocessing the data into a text corpus (#TOD0)


```{r}

```


# LDA (#TOD0)

```{r}
library(lda)
library(topicmodels)
library(tidytext)

doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(corpus)))
dtm <- DocumentTermMatrix(corpus[doc.lengths > 0])

#########################################################################################################

doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(ctweets)))
dtm <- DocumentTermMatrix(ctweets[doc.lengths > 0])

LDA_employ <- LDA(dtm, k =5,method = "VEM", cotrol = list(seed=1234))
employ_topics <- tidy(LDA_employ, matrix ="beta")

ap_top_terms <- employ_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
```

